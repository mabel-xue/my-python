import time
from bs4 import BeautifulSoup
import requests
import schedule

GROUP_URL = "https://www.douban.com/group/lala/?joining_pop=1"  # æ›¿æ¢ä¸ºä½ çš„è±†ç“£å°ç»„é“¾æ¥
COOKIES = {"bid": "mF6bMQ_KWno"}


HEADER_1 = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
}

HEADER_2 = {
    "Cookie": '__utmv=30149280.14513; _ga_RXNMP372GL=GS1.1.1711034587.1.1.1711034879.60.0.0; viewed="27049704_25717380_25833225_19952397_34861737_27108685_26862694_26289656_6439420_35124662"; bid=GiWey58i4VE; ll="108288"; _pk_id.100001.8cb4=2673e6c8bbfd9f90.1724073161.; __utmc=30149280; __utmz=30149280.1724073163.14.8.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; dbcl2="145133188:/kwCT+NrflY"; ck=nQze; frodotk_db="63f3c13cbb64dccf2bdb43b6322e966d"; ct=y; push_noty_num=0; push_doumail_num=0; _ga=GA1.2.1331631967.1711034371; _ga_Y4GN1R87RG=GS1.1.1724415931.4.0.1724415938.0.0.0; ap_v=0,6.0; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1725009468%2C%22https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3Dx7YmZcaHELVfZZHOMi64Omlhliq07Rh34jHHqQWPnfQjLInQz_nBVtrI_whjr3Wx%26wd%3D%26eqid%3Dc342dfc6000082460000000366c344bc%22%5D; _pk_ses.100001.8cb4=1; __utma=30149280.516241902.1682590651.1724680416.1725009469.24; __utmt=1; __utmb=30149280.75.4.1725010350149',
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
}


def get_soup(url, headers=None):
    rsp = requests.get(url, headers=headers)
    rsp.raise_for_status()
    html = rsp.text
    soup = BeautifulSoup(html, "html.parser")
    return soup


def check_group_and_comment():
    # åˆ·æ–°å°ç»„é¦–é¡µ
    soup = get_soup(GROUP_URL, HEADER_1)
    rows = soup.find("table", {"class": "olt"}).find_all("tr")
    print("start")
    for item in rows:
        titleDiv = item.find("td", {"class": "title"})
        if titleDiv:
            target = titleDiv.find("a")
            title = target.text
            link = target.get("href")
            if "ç¾¤æ‹›" in title and "æ‹¥æŠ±8" in title:
                print("æ‰¾åˆ°å¸–å­ï¼š", title)
                image_path = "/Users/mabelxue/Downloads/my-python/src/gm.jpeg"
                with open(image_path, "rb") as img:
                    # print(image_path)
                    comment_data = {
                        "rv_comment": "åŒ—äº¬GirlMatchå¿ƒåŠ¨è¯•éªŒâ€”â€”ç¬¬ä¸‰æœŸ\nâ— æ·±åº¦èŠå¤©ã€è°ˆè¯ã€å°æ¸¸æˆå¤šç»´åº¦ç¯èŠ‚ç©¿æ’\nâ— å…¨ç¨‹ä¸»æŒæ§åœºï¼Œiäººæåº¦é€‚é…\nâ— åœºåœ°å® ç‰©å‹å¥½ï¼Œç‹—å­ä¹Ÿèƒ½æ¬¢ä¹ç©è€\nâ— æ´»åŠ¨å†…å®¹è´¨é‡é«˜ï¼Œç¯èŠ‚é€æ¸å‡æ¸©ï¼Œå¸¦ä½ é¡ºåˆ©ç ´åœˆ\nğŸ“® æ´»åŠ¨æ—¶é—´ï¼š9-16ä¸‹åˆ14:00-19:00 ï¼ˆä¸­ç§‹ç¬¬äºŒå¤©ï¼‰\nğŸï¸ æ´»åŠ¨åœ°ç‚¹ï¼šä¸°å°åŒºæ§æ–°å…¬å›­\nğŸ‰ æ´»åŠ¨å†…å®¹æ¦‚è§ˆï¼š\nâ— é›†åˆç­¾åˆ°ã€æš–åœº\nâ— è¶£å‘³æ¸¸æˆç¬¬ä¸€è¶´\nâ— å®šå‘ä¸»é¢˜èŠå¤©äº¤æµ\nâ— è¶£å‘³æ¸¸æˆç¬¬äºŒè¶´\nâ— å¸®å¸®é—®\nğŸ™‹ğŸ»â€â™€ï¸ æ´»åŠ¨é¢„è®¡äººæ•°ï¼š12-15äºº å¹´é¾„è¦æ±‚ï¼š25+",
                        "start": "400",
                        "ck": "nQze",
                        "submit_btn": "å‘é€",
                    }
                    files = {"img": img}  # This is where the image file is added
                    comment_response = requests.post(
                        link + "/add_comment",
                        data=comment_data,
                        files=files,
                        headers=HEADER_2,
                    )
                    if comment_response.status_code == 200:
                        print("æˆåŠŸç•™è¨€")
                        return schedule.CancelJob

                    else:
                        print("ç•™è¨€å¤±è´¥", comment_response.text)


# è®¾ç½®å®šæ—¶ä»»åŠ¡
schedule.every(1).minutes.do(check_group_and_comment)  # æ¯10åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡

while True:
    schedule.run_pending()
    time.sleep(1)

# check_group_and_comment()
